{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# September 30 Meeting Markup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: \n",
    "\n",
    "This is the Plot.ly chart of the local histogram equalized Cocaine174 image:  \n",
    "https://neurodatadesign.github.io/seelviz/reveal/html/Cocaine174localeq.html\n",
    "\n",
    "This is the Plot.ly chart of the local histogram equalized atlas aligned Cocaine174 image:  \n",
    "https://neurodatadesign.github.io/seelviz/reveal/html/alignedCocaine174localeq.40000.brightest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue 1: Post-processing brains look like they have strange 'outline':\n",
    "After running histogram equalization on atlas-aligned brains, we get a significantly different shape (see Plot.ly charts above).\n",
    "\n",
    "**Reason:** My current CLAHE tile size in my local histogram equalization is not modular - this is partly because the tile size requires an integer argument, so it’s not possible to perfectly fit differently sized data. We have a function that calculates the best tile size depending on image size.\n",
    "\n",
    "Currently our method (see code snippet below) finds the volume of the brightest points and scales the tile size depending on that - the problem is that it rounds to the nearest integer.  We can think of a lot of fringe cases where this is not a standardized method; specifically for cases where the volume scaling unit is around 0.5, the scaling is **not** necessarily appropriate.  Our method for scaling the tile size depends on a “baseline” case using the CLAHE tile size of Cocaine174 purely based off of “what looks good”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code Snippet Showing Our CLAHE Tile Size Selection\n",
    "aligned_brain = nb.load('in.img')\n",
    "aligned_data = aligned_brain.get_data()\n",
    "aligned_brain = aligned_data[:,:,:]\n",
    "\n",
    "size = aligned_brain.shape\n",
    "\n",
    "totalTile = size[0]*size[1]*size[2] #<- Calculates total volume of the image\n",
    "print totalTile    #77045760\n",
    "scaleValue = totalTile/8\n",
    "print scaleValue    #9630720\n",
    "\n",
    "print float(totalTile/751091775.0)    #0.102578356686\n",
    "clahe_scale = 8*0.102578356686\n",
    "print clahe_scale    #0.820626853488"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Plot.ly chart of the local histogram equalized Cocaine174 image with a CLAHE tile size adjusted off of its volume based on the above method:\n",
    "https://neurodatadesign.github.io/seelviz/reveal/html/modularCLAHE.html\n",
    "\n",
    "Additional Notes: The larger concern we have is that the atlas aligned images seem to be different than the original brains - this is likely due to a theoretical misunderstanding we have about the aligned brains. They look different both in terms of how the original images look and in terms of the bright points. We define aligned brains to be the input for registration, so these are the pre-registered brains. Currently we are under the notion that the aligned images are simply transformed versions of the original brains - our assumption seems to be wrong because the images seem to have different brightness regions.\n",
    "\n",
    "# Questions: \n",
    "<span style=\"color: #000000; font-family: ; font-size: 14pt;\">\n",
    "1)  What is the best approach towards making the CLAHE tile size perfectly accurate for each image size?  </span>\n",
    "\n",
    "<span style=\"color: #000000; font-family: ; font-size: 14pt;\">\n",
    "2)  What do you suggest that we base the tile size off of - perhaps we should create a range of different tile sizes and look at the graph statistics for each one?  </span>\n",
    "\n",
    "<span style=\"color: #000000; font-family: ; font-size: 14pt;\">\n",
    "3)  Why do the atlas aligned images seem different in behavior from the original brains - both in how the original images look and in terms of the brightest points?   </span>\n",
    "\n",
    "<center> **Histogram Equalized Before Alignment** </center>\n",
    "![image3(histeqnonaligned)](jovoseptember30pngs/img3.png)\n",
    "<center> **Histogram Equalization After Alignment** </center>\n",
    "![image4(histeqaligned)](jovoseptember30pngs/img4.png)\n",
    "\n",
    "<center> **Raw Image of Histogram Equalized Before Alignment** </center>\n",
    "![image2(histeqnonaligned)](jovoseptember30pngs/img2.png)\n",
    "<center> **Raw Image of Histogram Equalization After Alignment** </center>\n",
    "![image7(histeqaligned)](jovoseptember30pngs/img7.png)\n",
    "\n",
    "# Questions (cont):\n",
    "<span style=\"color: #000000; font-family: ; font-size: 14pt;\">\n",
    "4)  After we finish the first prototype of the pipeline, what do you suggest we work on improving?  </span>\n",
    "\n",
    "<span style=\"color: #000000; font-family: ; font-size: 14pt;\">\n",
    "5)  Are there supposed to be around 368 regions in the brain?</span>\n",
    "\n",
    "## Issue 2: Pipeline\n",
    "Currently outputs all files and intermediaries into a folder.  Can be pip installed.\n",
    "![imagePipelineOutputs](jovoseptember30pngs/pipinstallclarityviz.jpg)\n",
    "\n",
    "After pip installation, can call functions from Python.  Does not implement latest color registration, since that’s a work in progress.\n",
    "\n",
    "![imagePipelineOutputs](jovoseptember30pngs/pipinstalledclarityviz.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
